########################################################################
Running the RInfo System Locally
########################################################################


Getting Started
========================================================================

It's recommended to run all components of the RInfo System on your local
machine during development. To do this from scratch, you need:

* Java JDK 1.6+ - <http://java.sun.com/javase/downloads/widget/jdk6.jsp>
* Maven 3+ - <http://maven.apache.org/>
* Tomcat 6+ - <http://tomcat.apache.org/>

To use the various instrumental tools used you also need:

* Groovy 2.0+ - <http://groovy.codehaus.org/Download>

* Python 2.7+ - <http://www.python.org/download/>

  - For management (such as deployment): see ``README.txt``.
  - For acceptance spec testing: see ``../documentation/spec/README.txt``
    (NOTE: these are peripheral right now, so you can skip this part).


Setting up the environment
========================================================================

Sesame
------------------------------------------------------------------------

You need a Java webapp container (e.g. Tomcat) to run the Sesame RDF
Repository server (aka openrdf-sesame). This server is expected to run
at ``http://localhost:8080/openrdf-sesame/``.

To build the sesame packages::

  $ cd packages/java/rinfo-sesame-http/
  $ mvn package

Then copy ``openrdf-sesame.war`` and (optionally)
``sesame-workbench.war`` found in ``target/dependency`` to the
container (e.g. ``$TOMCAT_HOME/webapps``).

On a linux system, unless you have configured anything else,
$TOMCAT_HOME is probably set to /var/lib/tomcat6/

Note: Tomcat is not really needed when running the services
standalone. See below under rinfo-service for an alternative way of
running the openrdf-sesame through through mvn and jetty.

ElasticSearch
------------------------------------------------------------------------

Download and install ElasticSearch by following the instructions at
<http://www.elasticsearch.org/>.

(For the time being, you need version 0.17.7, the very latest versions
don't work with the elasticsearch client lib that mvn pulls down from
the remote repositories)

Start an instance by running the ``elasticsearch`` command (commonly in the bin
directory of the installation)::

    $ elasticsearch -f

You can configure the location of log and data directories by creating an
``elasticsearch.yml`` file and adding something like this (choose paths to your
liking)::

    cluster:
        name: elasticsearch

    path:
        logs: /opt/work/rinfo/elasticsearch/log
        data: /opt/work/rinfo/elasticsearch/data

And run with (choose the config file location to your liking)::

    $ elasticsearch -f -D es.config=/opt/work/rinfo/elasticsearch/elasticsearch.yml


Data directories
------------------------------------------------------------------------

These directories must exist and be writable by the user used to run
maven and groovy. At start, they should be empty.

* /opt/work/rinfo/
* /opt/work/rinfo/depots/rinfo/

The RInfo Applications
------------------------------------------------------------------------

To run tools for building admin data and documentation, you need to configure
your local environment for them. See:

  - ``../tools/README.txt``

For details on setting this up.

Install the shared rinfo packages to your local maven repo by::

  $ cd packages/java
  $ mvn install

(``mvn install`` also runs all regression tests. If these fail for
some reason, the install process will not continue. See
packages/java/README.txt for hints on how to proceeed).

Now you are ready to start up:

* Create and serve integration test data:

  These steps will result in a web service that offers a feed of base
  data resources (RDF ontologies, basic RDF descriptions of shared
  resources such as organizations, and a list of other feeds that
  contain document and metadata) know as "the admin feed".

  In addition, the same web service offers (in separate feeds) some
  example data (PDF documents and RDF metadata about them).

  The web service is simply a small web server that serves static
  files from a particular directory.

  These can be created and served by the following steps::

    $ cd tools/rinfomain/
    $ ./map_feeds_and_base.sh

  Under hood, the ``map_feeds_and_base.sh`` script runs two groovy scripts with
  some present parameters for destination file paths.

  map_feeds_docs.groovy

    This creates a number of atom feed files, RDF files and document files (in
    PDF) under ``/opt/work/rinfo/testsources/www``. These are created from
    example files present in ``../../documentation``

  base_as_feed.groovy

    This creates files for the admin feed from the resources found in
    ``../../resources/base`` and the content feeds that now are present in
    ``/opt/work/rinfo/testsources/www``. The admin feed files are likewise
    created in ``/opt/work/rinfo/testsources/www``.

  Once these files are created, serve these using:

    $ groovy serve_folder.groovy

  This serves up the directory ``/opt/work/rinfo/testsources/www``
  Go to <http://localhost:8280/feed/current> to verify that it contains a
  valid feed. This contains the admin feed for the main application and
  a couple of test sources that the previous steps created.

  (These three scripts all take assorted command line flags if you
  wish to read resources and examples from other paths, or if you want
  to create the resulting files somewhere else. Read the source for
  details.)

* Start local instance of the RInfo system:

  rinfo-main

      Collects from the sources defined for development mode (a
      base/admin prototype feed and example suppliers) and supplies
      the main data feed. In order to do anything useful, rinfo-main
      needs a web service that contains the admin feed. The address of
      the admin feed is contained in the applications properties file,
      (normally ``src/environments/dev-unix/rinfo-main.properties``),
      see the property ``rinfo.main.collector.adminFeedUrl``.
      
      Go to the application package and run it::

        $ cd packages/java/rinfo-main/
        $ mvn jetty:run

      By default the profile (and thus environment settings) will set to one of
      the "dev-*" profiles. (See ``../packages/java/README.txt`` for details
      about environments.)

      In this mode, the application runs with scheduled collections turned
      *off*. To populate rinfo-main with data you must trigger collects
      manually::

        # Collect the admin feed
        curl -d 'feed=http://localhost:8280/feed/current' http://localhost:8180/collector

        # Collect the sources as pointed out in the admin feed
        curl -d 'feed=http://localhost:8280/arbetsformedlingen.se/current.atom' http://localhost:8180/collector
        curl -d 'feed=http://localhost:8280/boverket.se/current.atom' http://localhost:8180/collector
        curl -d 'feed=http://localhost:8280/jordbruksverket.se/current.atom' http://localhost:8180/collector
        curl -d 'feed=http://localhost:8280/slv.se/current.atom' http://localhost:8180/collector
        curl -d 'feed=http://localhost:8280/regeringen.se/current.atom' http://localhost:8180/collector
        curl -d 'feed=http://localhost:8280/komdir.regeringen.se/current.atom' http://localhost:8180/collector
        curl -d 'feed=http://localhost:8280/riksarkivet.se/current.atom' http://localhost:8180/collector
        curl -d 'feed=http://localhost:8280/verva.se/current.atom' http://localhost:8180/collector

      If you wish to automate this, the best way to enable scheduled
      collections is to create a local environment::

        $ mkdir src/environments/local/
        $ cp src/environments/dev-unix/*.properties src/environments/local/

      Edit ``rinfo-main.properties`` and make sure that the scheduleInterval is
      larger than -1, like this (for collecting every 60th second)::

        rinfo.main.collector.scheduleInterval=60

      Then run::

        $ mvn jetty:run -Plocal

      Verify that rinfo-main is up and running by going to
      http://localhost:8180/feed/current which should contain a valid
      feed. You can also verify that data has been collected by
      looking in the directories under
      ``/opt/work/rinfo/depots/rinfo/depot/``, particularly ``publ``
      and ``feed``.

      Additional information can be found in:
      ``../packages/java/rinfo-main/README.txt``.

  rinfo-service

      Collects data from rinfo-main into the sesame RDF repo, making
      data available for its data views. In order to run rinfo-service
      you need to first have a running sesame server started. This can
      be done either by running openrdf-sesame in a Tomcat server, as
      described above, or by running running "mvn jetty:run-war" in
      the rinfo-sesame-http directory. After that you can start
      rinfo-service by running "mvn jetty:run" from the rinfo-service
      directory.

      You also need to have elasticsearch running. See instructions
      above.
      
      To populate rinfo-service with data you must trigger a collect of
      rinfo-main::

        $ curl -d 'feed=http://localhost:8180/feed/current' http://localhost:8181/collector

      (If you have a local setting for enabled scheduled collects, as described
      above, you can instead also add the following in your
      ``rinfo-main.properties``::

        rinfo.main.collector.onCompletePingTargets=http://localhost:8181/collector

      This will have main automatically ping service instead.)

      Verify that rinfo-service is up and running by going to
      http://localhost:8181/ui/ where you should be able to search and navigate
      using the service REST API.

      Additional information can be found in:
      ``../packages/java/rinfo-service/README.txt``.


Starting out clean
========================================================================

[How to wipe rinfo-main, elasticsearch and sesame]

1. rinfo-main: rm -r /opt/work/rinfo/depots/rinfo

Adding source feeds
========================================================================

[How to add a source feed to the admin feed, and collect it]

1. Edit /opt/work/rinfo/testsources/www/sys/sources/rdf.rdf 
      
In Depth
========================================================================

To get an *understanding* of how a staging or production environment is set up,
you can study the deployment script in ``fabfile.py`` and the scripts it
includes to provide setup and deployment commands.

